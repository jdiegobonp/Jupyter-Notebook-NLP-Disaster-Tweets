{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56c4abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "29c1b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mafit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0c38de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mafit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mafit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "af6d11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75ee3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reducir caracteres repetidos (nooooo -> noo)\n",
    "def reduce_repeated_chars(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18f444b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta= \"train.csv\"\n",
    "df= pd.read_csv(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0db619eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cfbb4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   keyword  7552 non-null   object\n",
      " 1   text     7613 non-null   object\n",
      " 2   target   7613 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 178.6+ KB\n",
      "None\n",
      "\n",
      "Primeras filas:\n",
      "  keyword                                               text  target\n",
      "0     NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1     NaN             Forest fire near La Ronge Sask. Canada       1\n",
      "2     NaN  All residents asked to 'shelter in place' are ...       1\n",
      "3     NaN  13,000 people receive #wildfires evacuation or...       1\n",
      "4     NaN  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "\n",
      "Nulos por columna:\n",
      "keyword    61\n",
      "text        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Distribución de target:\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Información general:\")\n",
    "print(df.info())\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Revisar nulos\n",
    "print(\"\\nNulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Ver distribución de la variable target\n",
    "print(\"\\nDistribución de target:\")\n",
    "print(df['target'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e9db064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenar nulos de keyword y location\n",
    "df['keyword'] = df['keyword'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99f74bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar texto (limpieza general)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = reduce_repeated_chars(text)  # Ejemplo: nooooo -> noo\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # Quitar URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Quitar menciones\n",
    "    text = re.sub(r'#', '', text)  # Quitar hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Solo letras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Espacios extra\n",
    "    \n",
    "    # Lematización (convertir a la raíz de la palabra)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar al dataframe\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Preprocesar keyword (limpieza general)\n",
    "def clean_kw(keyword):\n",
    "    keyword = keyword.lower()\n",
    "    keyword = reduce_repeated_chars(keyword)  # Ejemplo: goooooal -> goal\n",
    "    keyword = re.sub(r'http\\S+|www.\\S+', '', keyword)\n",
    "    keyword = re.sub(r'@\\w+', '', keyword)\n",
    "    keyword = re.sub(r'#', '', keyword)\n",
    "    keyword = re.sub(r'[^a-zA-Z\\s]', '', keyword)\n",
    "    keyword = re.sub(r'\\s+', ' ', keyword).strip()\n",
    "    keyword = re.sub(r'%20', ' ', keyword).strip()\n",
    "    keyword = re.sub(r'20', ' ', keyword).strip()\n",
    "    \n",
    "    # Lematización de keywords también\n",
    "    words = keyword.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar al dataframe\n",
    "df['keyword'] = df['keyword'].apply(clean_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1fdf9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stopwords personalizadas (preservando términos de desastres)\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "disaster_terms = {\n",
    "    'fire', 'flood', 'earthquake', 'hurricane', 'storm', 'tornado', 'tsunami', \n",
    "    'explosion', 'disaster', 'emergency', 'evacuate', 'help', 'rain', 'smoke', \n",
    "    'wind', 'eruption', 'ablaze', 'floods', 'flooding', 'quake', 'aftershock',\n",
    "    'eruption', 'wildfire', 'landslide', 'mudslide', 'volcano', 'tremor',\n",
    "    'typhoon', 'cyclone', 'avalanche', 'blizzard', 'heatwave', 'drought', \n",
    "    'sinkhole', 'hailstorm', 'sandstorm', 'firestorm', 'catastrophe', \n",
    "    'calamity', 'collapse', 'rescue', 'survivor', 'emergency', 'alert',\n",
    "    'crisis', 'evacuation', 'shelter', 'stormsurge', 'firenado', 'flashflood',\n",
    "    'aftershocks', 'earthquacke', 'hurricance', 'huricane', 'wildfires', \n",
    "    'landslides', 'firenados', 'cyclones', 'typhoons', 'tsunamis', 'avalanches',\n",
    "    'disaters', 'eruption', 'erthquake', 'emergncy', 'emerjency', 'disasterous',\n",
    "    'volcanic', 'epicenter', 'shockwave', 'tremblor', 'blackout', 'poweroutage',\n",
    "    'hazmat', 'biohazard', 'collapse', 'derailment', 'hazard', 'sinkholes',\n",
    "    'tsnuami', 'firefighters', 'firefighter', 'firstresponders', 'firstresponder',\n",
    "    'searchandrescue', 'relief', 'aftermath', 'emergencyresponse', \n",
    "    'hazardous', 'catastrophic', 'prevention', 'preparedness', 'evacuated',\n",
    "    'fatalities', 'casualties', 'displaced', 'sos', 'mayday', 'helpme', 'needhelp',\n",
    "    'emergancy', 'volcan', 'smokecloud', 'ashfall', 'stormwatch', 'disasterrelief',\n",
    "    'calamities', 'seismic', 'quakes', 'weatheralert', 'weatherwarning', \n",
    "    'damage', 'destroyed', 'devastation', 'wreckage', 'firenation', \n",
    "    'explosions', 'explosiv', 'earthquakes', 'floodwaters'\n",
    "}\n",
    "\n",
    "custom_stopwords = [word for word in default_stopwords if word not in disaster_terms]\n",
    "\n",
    "def remove_custom_stopwords(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in custom_stopwords])\n",
    "\n",
    "df['text'] = df['text'].apply(remove_custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0974fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3dbcf363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"prueba.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c7077aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar keyword, location y text en un solo campo (para vectorización o embeddings)\n",
    "df['full_text'] = df['keyword'] + ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bbbe2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar texto (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.95)\n",
    "X = vectorizer.fit_transform(df['full_text'])\n",
    "\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bff4c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiamos test para posterior prueba\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Asegúrate de aplicar la misma limpieza que al train (por consistencia)\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_test['keyword'] = df_test['keyword'].fillna('unknown')\n",
    "df_test['location'] = df_test['location'].fillna('unknown')\n",
    "df_test['location'] = df_test['location'].str.lower().str.strip()\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(clean_text)\n",
    "df_test['full_text'] = df_test['keyword'] + ' ' + df_test['location'] + ' ' + df_test['text']\n",
    "\n",
    "# Vectorizar usando el mismo vectorizer que usaste en el train\n",
    "X_test = vectorizer.transform(df_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c727676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARAMOS LOS VALORES PARA ENTRENAR LA REGRESION\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "616012b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "700450c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       874\n",
      "           1       0.80      0.71      0.75       649\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MODELO REGRESION\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61659550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARACION PARA BERT\n",
    "\n",
    "X = df['full_text'].tolist()\n",
    "y = df['target'].values\n",
    "\n",
    "# Split train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fc5d633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos físicos: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Dispositivos físicos:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f1ebba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No se detectó GPU, corriendo en CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 60\u001b[0m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     54\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     55\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     56\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_enc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[0;32m     66\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo y predecir en validación\u001b[39;00m\n\u001b[0;32m     69\u001b[0m y_val_pred_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_enc)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tf_keras\\src\\engine\\training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1802\u001b[0m ):\n\u001b[0;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:906\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[0;32m    911\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[0;32m    912\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import AdamWeightDecay\n",
    "from sklearn.metrics import classification_report\n",
    "from tf_keras import layers, models\n",
    "\n",
    "# Forzar uso de GPU si está disponible\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"✅ Usando GPU: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠️ No se detectó GPU, corriendo en CPU.\")\n",
    "# Cargar el tokenizador BERT (puedes cambiar el modelo si prefieres otro idioma o versión)\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Función para tokenizar textos (BERT necesita input_ids y attention_mask)\n",
    "def encode_texts(texts, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "# Tokenizar train y validation\n",
    "X_train_enc = encode_texts(X_train, tokenizer)\n",
    "X_val_enc = encode_texts(X_val, tokenizer)\n",
    "\n",
    "# Convertir las etiquetas a tensores\n",
    "y_train_enc = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "y_val_enc = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
    "\n",
    "# Cargar modelo preentrenado BERT para clasificación binaria (2 etiquetas: 0 y 1)\n",
    "model = TFBertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)\n",
    "\n",
    "# Definir optimizador (AdamWeightDecay es recomendado por Hugging Face)\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "\n",
    "# Compilar modelo con pérdida para clasificación binaria\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    X_train_enc,\n",
    "    y_train_enc,\n",
    "    validation_data=(X_val_enc, y_val_enc),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Evaluar el modelo y predecir en validación\n",
    "y_val_pred_logits = model.predict(X_val_enc).logits\n",
    "y_val_pred = np.argmax(y_val_pred_logits, axis=1)\n",
    "\n",
    "# Métricas de validación\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "755e4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en predictions_logreg.xlsx\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# Guardar predicciones\n",
    "df_test['target'] = y_test_pred\n",
    "df_test[['id', 'target']].to_excel('predictions_logreg.xlsx', index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en predictions_logreg.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a3f859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 212s 2s/step\n",
      "   id  bert_pred\n",
      "0   0          1\n",
      "1   2          1\n",
      "2   3          1\n",
      "3   9          1\n",
      "4  11          1\n",
      "Predicciones guardadas en 'bert_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Tokenizar el test set\n",
    "X_test_texts = df_test['full_text'].tolist()\n",
    "\n",
    "# Tokenizar usando la misma función y tokenizador\n",
    "X_test_enc = encode_texts(X_test_texts, tokenizer)\n",
    "\n",
    "# Paso 2: Hacer predicciones con el modelo BERT entrenado\n",
    "y_test_pred_logits = model.predict(X_test_enc).logits\n",
    "y_test_pred = np.argmax(y_test_pred_logits, axis=1)\n",
    "\n",
    "# Paso 3: Guardar predicciones en el DataFrame\n",
    "df_test['bert_pred'] = y_test_pred\n",
    "\n",
    "# Mostrar un preview de los resultados\n",
    "print(df_test[['id', 'bert_pred']].head())\n",
    "\n",
    "# (Opcional) Guardar predicciones a un CSV\n",
    "df_test[['id', 'bert_pred']].rename(columns={'bert_pred': 'target'}).to_csv('bert_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en 'bert_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7e9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
